{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "\n",
    "from torchvision import models, datasets\n",
    "from torchvision import transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import copy\n",
    "from torch.optim import lr_scheduler\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying transforms to the data\n",
    "#Statistics Based on ImageNet Data for Normalisation\n",
    "mean_nums = [0.485, 0.456, 0.406]\n",
    "std_nums = [0.229, 0.224, 0.225]\n",
    "\n",
    "image_transforms = {\"train\":transforms.Compose([\n",
    "                                transforms.Resize((128,128)), #Resizes all images into same dimension\n",
    "                                transforms.RandomRotation(10), # Rotates the images upto Max of 10 Degrees\n",
    "                                transforms.RandomHorizontalFlip(p=0.4), #Performs Horizantal Flip over images \n",
    "                                transforms.ToTensor(), # Coverts into Tensors\n",
    "                                transforms.Normalize(mean = mean_nums, std=std_nums)]), # Normalizes\n",
    "                    \"valid\": transforms.Compose([\n",
    "                                transforms.Resize((128,128)),\n",
    "                                transforms.CenterCrop(150), #Performs Crop at Center and resizes it to 150x150\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=mean_nums, std = std_nums)\n",
    "                    ])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# Set train, valid, and test directory\n",
    "def create_data(bs,n):\n",
    "    \n",
    "    \n",
    "\n",
    "    train_directory = 'chest_xray/train'\n",
    "    valid_directory = 'chest_xray/test'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    data = {\n",
    "        'train': datasets.ImageFolder(root=train_directory,\n",
    "                                    transform=image_transforms['train']),\n",
    "        'valid': datasets.ImageFolder(root=valid_directory,\n",
    "                                    transform=image_transforms['valid']),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    # size of data, to be used for calculating Averge Loss and Accuracy\n",
    "    train_data_size = len(data['train'])\n",
    "    valid_data_size = len(data['valid'])\n",
    "\n",
    "    # Convert labels to 0s and 1s\n",
    "    class_to_idx = data['train'].class_to_idx\n",
    "    \n",
    "    data['train'].targets = [1 if data['train'].targets[i] == class_to_idx['PNEUMONIA'] else 0 for i in range(len(data['train'].targets))]\n",
    "    data['valid'].targets = [1 if data['valid'].targets[i] == class_to_idx['PNEUMONIA'] else 0 for i in range(len(data['valid'].targets))]\n",
    "\n",
    "    \n",
    "\n",
    "    # Create iterators for the Data loaded using DataLoader module\n",
    "    train_data = DataLoader(data['train'], batch_size=bs, shuffle=True)\n",
    "    valid_data = DataLoader(data['valid'], batch_size=1, shuffle=True)\n",
    "\n",
    "    \n",
    "    hold=[]\n",
    "    trainloader=[]\n",
    "    testloader=[]\n",
    "\n",
    "    k=(train_data_size/bs)/(n)\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_data):\n",
    "        hold.append([inputs,labels])\n",
    "        \n",
    "        if i % int(k) == 0 and i != 0:\n",
    "            \n",
    "            trainloader.append(hold)\n",
    "            hold=[]\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(valid_data):\n",
    "        testloader.append([inputs,labels])\n",
    "\n",
    "            \n",
    "\n",
    "    return trainloader, testloader, valid_data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelA, self).__init__()\n",
    "        self.resnet50 = models.densenet121(pretrained=True)\n",
    "        self.act1 = nn.Sigmoid() \n",
    "        for param in self.resnet50.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.fc1 = nn.Linear(1000, 512)\n",
    "    def forward(self, x):\n",
    "        x = self.resnet50(x)\n",
    "        output = self.act1(self.fc1(x))\n",
    "        return output\n",
    "\n",
    "class ModelB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelB, self).__init__()\n",
    "\n",
    "\n",
    "        \n",
    "        # Create an instance of ModelA\n",
    "        self.act1 = nn.Sigmoid()  \n",
    "        # Additional layers for ModelB (classification or other operations)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get the output of ModelA\n",
    "\n",
    "        x = self.act1(self.fc2(x))\n",
    "        x = self.act1(self.fc3(x))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ModelC(nn.Module):\n",
    "    def __init__(self, modelA, modelB):\n",
    "        super(ModelC, self).__init__()\n",
    "        self.model_A = modelA\n",
    "        self.model_B = modelB\n",
    "        self.classification_layer = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through ModelA and then ModelB\n",
    "        x = self.model_A(x)\n",
    "        x = self.model_B(x)\n",
    "        x = self.classification_layer(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train(model, epochs, train_data, lr):\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_data):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Ensure labels are in the right format (adjust if needed)\n",
    "            # For example, if outputs are logits, convert labels to one-hot encoding\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_func(outputs.squeeze(), labels.float())  # Assuming outputs are logits\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(model, val_data, val_size):\n",
    "\n",
    "    loss_func = nn.BCEWithLogitsLoss()  # Use BCEWithLogitsLoss for binary classification\n",
    "\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for j, (inputs, labels) in enumerate(val_data):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)  # Ensure labels are in the correct shape\n",
    "\n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_func(outputs, labels)  # Calculate BCE loss\n",
    "            valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            predictions = (torch.sigmoid(outputs) >= 0.5).float()  # Convert logits to binary predictions (0 or 1)\n",
    "            correct_counts = (predictions == labels).sum().item()\n",
    "            valid_acc += correct_counts\n",
    "\n",
    "    return valid_loss / val_size, valid_acc / val_size\n",
    "\n",
    "\n",
    "      \n",
    "           \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_model(n):\n",
    "    model_list=[]\n",
    "    for i in range(n):\n",
    "        modelA_instance = ModelA()\n",
    "        modelB_instance = ModelB()  \n",
    "\n",
    "        start_model = ModelC(modelA_instance, modelB_instance)\n",
    "     \n",
    "        model_list.append(start_model)\n",
    "    return model_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_global_standard(n,k,n_epochs,batch_size):\n",
    "\n",
    "    trainloaders,val_data,val_size=create_data(batch_size,n)\n",
    "    \n",
    "\n",
    "\n",
    "    mod_list=num_model(n)\n",
    "    acc=[]\n",
    "    epo=[]\n",
    "    curr=0\n",
    "    \n",
    "    best_value=0\n",
    "    for i in range(k):\n",
    "\n",
    "        for z in range(n):\n",
    "            train(mod_list[z],n_epochs,trainloaders[z],0.001)\n",
    "\n",
    "        epo.append(curr)\n",
    "        curr=curr+1\n",
    "        print(curr)\n",
    "\n",
    "\n",
    "        res_acc=[]  \n",
    "        res_loss=[]        \n",
    "        for i in range(n):\n",
    "            hold=test(mod_list[i],val_data,val_size)\n",
    "            print(hold)\n",
    "            res_acc.append(hold[1])\n",
    "            res_loss.append(hold[0])\n",
    "\n",
    "        print(res_acc)\n",
    "        avg_value=sum(res_acc)/n\n",
    "        avg_loss=sum(res_loss)/n\n",
    "        print(f'avrage value {avg_value}')\n",
    "\n",
    "        acc.append([avg_value,avg_loss])  \n",
    "  \n",
    "        if avg_value > best_value:\n",
    "            best_value = avg_value\n",
    "\n",
    "        \n",
    "        parameter_lists=[]\n",
    "        for model in mod_list:\n",
    "            hold=model.model_B\n",
    "            parameter_lists.append(hold.parameters())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        avg_params = [torch.mean(torch.stack(params), dim=0) for params in zip(*parameter_lists)]\n",
    "\n",
    "            \n",
    "\n",
    "        glob_hold = ModelB()\n",
    "\n",
    "            \n",
    "        updated_params = []\n",
    "\n",
    "            \n",
    "        for glob_param, avg_param in zip(glob_hold.parameters(), avg_params):\n",
    "            updated_params.append(avg_param)\n",
    "\n",
    "        \n",
    "        for glob_param, updated_param in zip(glob_hold.parameters(), updated_params):\n",
    "            glob_param.data.copy_(updated_param)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        for model_instance in mod_list:\n",
    "            model_instance.model_B = glob_hold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.plot(epo,acc,label=('Average Accuracy',\"Average Loss\"))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.title(\"Convergence of Federated Learning with Five Models (5 Epochs, Batch Size 32)\")\n",
    "    plt.legend()\n",
    "    plt.axes([1, 0.4, 0.2, 0.2])\n",
    "    plt.boxplot(res_acc)\n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.title('Boxplot of Accuracy')\n",
    "    plt.show()\n",
    "    return best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_global_standard(5,50,5,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_global_Imp(n,k,n_epochs,batch_size):\n",
    "\n",
    "    trainloaders,val_data,val_size=create_data(batch_size,n)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    mod_list=num_model(n)\n",
    "    for i in range(n):\n",
    "        train(mod_list[i],100,trainloaders[i],0.001)\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    best_avg_acc=0\n",
    "    acc=[]\n",
    "    epo=[]\n",
    "    curr=0\n",
    "    for i in range(k):\n",
    "\n",
    "        for i in range(n):\n",
    "            train(mod_list[i],n_epochs,trainloaders[i],0.001)\n",
    "\n",
    "        epo.append(curr)\n",
    "        curr=curr+1\n",
    "        print(curr)\n",
    "\n",
    "        res_acc=[]  \n",
    "        res_loss=[]        \n",
    "        for i in range(n):\n",
    "            hold=test(mod_list[i],val_data,val_size)\n",
    "            print(hold)\n",
    "            res_acc.append(hold[1])\n",
    "            res_loss.append(hold[0])\n",
    "\n",
    "\n",
    "        avg_value=sum(res_acc)/n\n",
    "        avg_loss=sum(res_loss)/n\n",
    "        print(f'avrage value {avg_value}')\n",
    "\n",
    "        acc.append([avg_value,avg_loss])    \n",
    "\n",
    "        if avg_value > best_avg_acc:\n",
    "            best_avg_acc = avg_value\n",
    "        parameter_lists=[]\n",
    "        k=0\n",
    "        for model in mod_list:\n",
    "            if res_acc[k]>avg_value:\n",
    "                print(\"fedder\")\n",
    "                hold=model.model_B\n",
    "                parameter_lists.append(hold.parameters())\n",
    "            k=k+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        avg_params = [torch.mean(torch.stack(params), dim=0) for params in zip(*parameter_lists)]\n",
    "\n",
    "            \n",
    "\n",
    "        glob_hold = ModelB()\n",
    "\n",
    "            \n",
    "        updated_params = []\n",
    "\n",
    "            \n",
    "        for glob_param, avg_param in zip(glob_hold.parameters(), avg_params):\n",
    "            updated_params.append(avg_param)\n",
    "\n",
    "        \n",
    "        for glob_param, updated_param in zip(glob_hold.parameters(), updated_params):\n",
    "            glob_param.data.copy_(updated_param)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        for model_instance in mod_list:\n",
    "            model_instance.model_B = glob_hold\n",
    "\n",
    "        # Freeze all layers in glob_hold\n",
    "        if curr % 5 == 0:\n",
    "            print(\"fixer\")\n",
    "            \n",
    "            for param in glob_hold.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "\n",
    "            for model_instance in mod_list:\n",
    "                model_instance.model_B = glob_hold\n",
    "                \n",
    "\n",
    "      \n",
    "            k=0\n",
    "            for i in range(n):\n",
    "                if res_acc[k]<avg_value:\n",
    "                    print(\"fed_fix\")\n",
    "                    train(mod_list[i],100,trainloaders[i],0.003)\n",
    "                k=k+1\n",
    "\n",
    "            for model_instance in mod_list:\n",
    "                for param in model_instance.parameters():\n",
    "                    param.requires_grad = True\n",
    "                \n",
    "\n",
    "            print(test(mod_list[i],val_data,val_size))\n",
    "        else:\n",
    "            for model_instance in mod_list:\n",
    "                model_instance.model_B = glob_hold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "         \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    plt.plot(epo,acc,label=('Average Accuracy',\"Average Loss\"))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.title(\"Convergence of Federated Learning with Five Models (5 Epochs, Batch Size 32)\")\n",
    "    plt.legend()\n",
    "    plt.axes([1, 0.4, 0.2, 0.2])\n",
    "    plt.boxplot(res_acc)\n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.title('Boxplot of Accuracy')\n",
    "    plt.show()\n",
    "    return best_avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_global_Imp(5,50,5,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Fed(n,Itterations,n_epochs,batch_size):\n",
    "\n",
    "    mod_list=num_model(n)\n",
    "    \n",
    "    trainloaders,val_data,val_size=create_data(batch_size,n)\n",
    "    print(len(trainloaders))\n",
    "    modelA_instance = ModelA()\n",
    "    modelB_instance = ModelB()  \n",
    "\n",
    "    glob_model = ModelC(modelA_instance, modelB_instance)\n",
    "    acc=[]\n",
    "    epo=[]\n",
    "    curr=0\n",
    "    best_value=0\n",
    "    for i in range(Itterations):\n",
    "\n",
    "        for i in range(n):\n",
    "            train(mod_list[i],n_epochs,trainloaders[i],0.001)\n",
    "\n",
    "        parameter_lists = [model.parameters() for model in mod_list]\n",
    "\n",
    "        avg_params = [torch.mean(torch.stack(params), dim=0) for params in zip(*parameter_lists)]\n",
    "\n",
    "        for glob_param, param in zip(glob_model.parameters(), avg_params):\n",
    "            glob_param.data.copy_(param)\n",
    "\n",
    "        mod_list=[]\n",
    "\n",
    "        for i in range(n):\n",
    "            mod_list.append(glob_model)\n",
    "\n",
    "\n",
    "        epo.append(curr)\n",
    "        curr=curr+1\n",
    "        print(curr)\n",
    "\n",
    "        res_acc=[]  \n",
    "        res_loss=[]        \n",
    "        for i in range(1):\n",
    "            hold=test(mod_list[i],val_data,val_size)\n",
    "            print(hold)\n",
    "            res_acc.append(hold[1])\n",
    "            res_loss.append(hold[0])\n",
    "\n",
    "        \n",
    "\n",
    "        avg_value=sum(res_acc)\n",
    "        avg_loss=sum(res_loss)\n",
    "\n",
    "        if avg_value>best_value:\n",
    "            best_value=avg_value\n",
    "\n",
    "\n",
    "        print(f'avrage value {avg_value}')\n",
    "\n",
    "        acc.append([avg_value,avg_loss])\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    plt.plot(epo,acc,label=('Average Accuracy',\"Average Loss\"))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.title(\"Accuracy of Image Categorization\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    print(best_value)\n",
    "    return best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Fed(1,20,5,32)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FedAvg",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
