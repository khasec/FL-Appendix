{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import copy\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def creat_data(n):\n",
    "    data = pd.read_csv(\"Heart Attack.csv\")\n",
    "    data_start = pd.read_csv(\"Heart Attack.csv\")\n",
    "    df_chunks = np.array_split(data_start, n)\n",
    "\n",
    "    trainloaders = []\n",
    "    testloaders = []\n",
    "\n",
    "    X = data.drop(columns='class')\n",
    "    y = data['class']\n",
    "\n",
    "    X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "    X_train1 = torch.Tensor(X_train1.values)\n",
    "    y_train1 = torch.Tensor(y_train1.values)\n",
    "    X_train1 = X_train1.to(dtype=torch.float32).clone().detach()\n",
    "    y_train1 = y_train1.to(dtype=torch.float32).reshape(-1, 1).clone().detach()\n",
    "\n",
    "    X_train1 = X_train1.to(device)\n",
    "    y_train1 = y_train1.to(device)\n",
    "\n",
    "    X_test1 = torch.Tensor(X_test1.values)\n",
    "    y_test1 = torch.Tensor(y_test1.values)\n",
    "    X_test1 = X_test1.to(dtype=torch.float32).clone().detach()\n",
    "    y_test1 = y_test1.to(dtype=torch.float32).reshape(-1, 1).clone().detach()\n",
    "\n",
    "    X_test1 = X_test1.to(device)\n",
    "    y_test1 = y_test1.to(device)\n",
    "\n",
    "    for data in df_chunks:\n",
    "        hold1 = []\n",
    "        hold2 = []\n",
    "\n",
    "        X = data.drop(columns='class')\n",
    "        y = data['class']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "        X_train = torch.Tensor(X_train.values)\n",
    "        y_train = torch.Tensor(y_train.values)\n",
    "        X_train = X_train.to(dtype=torch.float32).clone().detach()\n",
    "        y_train = y_train.to(dtype=torch.float32).reshape(-1, 1).clone().detach()\n",
    "\n",
    "        X_train = X_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "\n",
    "        X_test = torch.Tensor(X_test.values)\n",
    "        y_test = torch.Tensor(y_test.values)\n",
    "        X_test = X_test.to(dtype=torch.float32).clone().detach()\n",
    "        y_test = y_test.to(dtype=torch.float32).reshape(-1, 1).clone().detach()\n",
    "\n",
    "        X_test = X_test.to(device)\n",
    "        y_test = y_test.to(device)\n",
    "\n",
    "        hold1.append((X_train, y_train))\n",
    "        hold2.append((X_test, y_test))\n",
    "        trainloaders.append(hold1)\n",
    "        testloaders.append(hold2)\n",
    "\n",
    "    return trainloaders, testloaders, X_train1, y_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class ModelA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(8, 8)    \n",
    "        self.hidden2 = nn.Linear(8, 16)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "        self.hidden3 = nn.Linear(16, 32)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "        self.hidden4 = nn.Linear(32, 16)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "        self.hidden5 = nn.Linear(16, 8)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "        self.hidden6 = nn.Linear(8, 4)\n",
    "        self.output= nn.Linear(4, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act2(self.hidden3(x))\n",
    "        x = self.act2(self.hidden4(x))\n",
    "        x = self.act2(self.hidden5(x))\n",
    "        x = self.act2(self.hidden6(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelB(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ModelB, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(8, 8)\n",
    "        self.act1 = nn.Sigmoid() \n",
    "        self.model = model# Insert Model A in between input and output\n",
    "        \n",
    "        self.output= nn.Linear(4, 1)\n",
    "\n",
    "        self.act_output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.input_layer(x))\n",
    "        x = self.model(x)  # Pass through Model A\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x\n",
    "    \n",
    "    def give(self):\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,n_epochs,batch_size,traindata,lr):\n",
    "    X_train=traindata[0][0]\n",
    "    model=model.to(device)\n",
    "    y_train=traindata[0][1]\n",
    "    loss_func = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    for epoch in range(n_epochs):\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                Xbatch = X_train[i:i+batch_size]\n",
    "                y_pred = model(Xbatch)\n",
    "                ybatch = y_train[i:i+batch_size]\n",
    "                loss = loss_func(y_pred, ybatch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "\n",
    "            if epoch % 500 == 0 & epoch != 0:\n",
    "                with torch.no_grad():\n",
    "                    y_pred = model(X_train)\n",
    "\n",
    "                accuracy = (y_pred.round() == y_train).float().mean()\n",
    "                with torch.no_grad():\n",
    "                    y_pred = model(X_train)\n",
    "\n",
    "\n",
    "\n",
    "                print(f\"Accuracy {accuracy}\")\n",
    "                print(f'Finished epoch {epoch}, latest loss {loss}')\n",
    "\n",
    "\n",
    "def test(model, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    model=model.to(device)\n",
    "    X_test=testloader[0][0]\n",
    "    \n",
    "    y_test=testloader[0][1]\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    loss_func = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "    accuracy = (y_pred.round() == y_test).float().mean()\n",
    "\n",
    "    loss = loss_func(y_pred, y_test)\n",
    "\n",
    "    return loss.tolist(), accuracy.tolist()\n",
    "    \n",
    "\n",
    "\n",
    "def test_whole_data(model, x,y):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    model=model.to(device)\n",
    "    X_test=x\n",
    "    y_test=y\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    loss_func = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "    accuracy = (y_pred.round() == y_test).float().mean()\n",
    "\n",
    "    loss = loss_func(y_pred, y_test)\n",
    "\n",
    "    return loss.tolist(), accuracy.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_model(n):\n",
    "    model_list=[]\n",
    "    for i in range(n):\n",
    "        model1=ModelA()\n",
    "\n",
    "\n",
    "        model2=ModelB(model1)\n",
    "     \n",
    "        model_list.append(model2)\n",
    "    return model_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_global_standard(n,k,n_epochs,batch_size):\n",
    "\n",
    "    mod_list=num_model(n)\n",
    "    trainloaders,testloaders,x_test,y_test=creat_data(n)\n",
    "\n",
    "    model1=ModelA()\n",
    "\n",
    "\n",
    "    glob_model=ModelB(model1)\n",
    "    itterate=True\n",
    "    acc=[]\n",
    "    epo=[]\n",
    "    curr=0\n",
    "    best_avg_acc=0\n",
    "    \n",
    "    for i in range(k):\n",
    "\n",
    "        for i in range(n):\n",
    "            train(mod_list[i],n_epochs,batch_size,trainloaders[i],lr=0.001)\n",
    "\n",
    "        epo.append(curr)\n",
    "        curr=curr+1\n",
    "        print(curr)\n",
    "\n",
    "        res_acc=[]  \n",
    "        res_loss=[]        \n",
    "        for i in range(n):\n",
    "            hold=test_whole_data(mod_list[i],x_test,y_test)\n",
    "            print(hold)\n",
    "            res_acc.append(hold[1])\n",
    "            res_loss.append(hold[0])\n",
    "\n",
    "\n",
    "        avg_value=sum(res_acc)/n\n",
    "        avg_loss=sum(res_loss)/n\n",
    "        print(f'avrage value {avg_value}')\n",
    "\n",
    "\n",
    "        if avg_value > best_avg_acc:\n",
    "            best_avg_acc = avg_value\n",
    "        acc.append([avg_value,avg_loss])  \n",
    "\n",
    "        parameter_lists=[]\n",
    "        for model in mod_list:\n",
    "            hold=model.give()\n",
    "            parameter_lists.append(hold.parameters())\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        avg_params = [torch.mean(torch.stack(params), dim=0) for params in zip(*parameter_lists)]\n",
    "\n",
    "        glob_hold = ModelA() \n",
    "\n",
    "        \n",
    "        updated_params = []\n",
    "\n",
    "        \n",
    "        for glob_param, avg_param in zip(glob_hold.parameters(), avg_params):\n",
    "            updated_params.append(avg_param)\n",
    "\n",
    "       \n",
    "        for glob_param, updated_param in zip(glob_hold.parameters(), updated_params):\n",
    "            glob_param.data.copy_(updated_param)\n",
    "\n",
    "        \n",
    "\n",
    "        for model_instance in mod_list:\n",
    "            model_instance.model = glob_hold\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    plt.plot(epo,acc,label=('Average Accuracy',\"Average Loss\"))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.title(\"Convergence of Federated Learning with Five Models (5 Epochs, Batch Size 32)\")\n",
    "    plt.legend()\n",
    "    plt.axes([1, 0.4, 0.2, 0.2])\n",
    "    plt.boxplot(res_acc)\n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.title('Boxplot of Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    return best_avg_acc\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_global_standard(5,5000,5,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_global_Imp(n,k,n_epochs,batch_size):\n",
    "\n",
    "    trainloaders,testloaders,x_test,y_test=creat_data(n)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    mod_list=num_model(n)\n",
    "    for i in range(n):\n",
    "        train(mod_list[i],1000,batch_size,trainloaders[i],lr=0.001)\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    best_avg_acc=0\n",
    "    acc=[]\n",
    "    epo=[]\n",
    "    curr=0\n",
    "    for i in range(k):\n",
    "\n",
    "        for i in range(n):\n",
    "            train(mod_list[i],n_epochs,batch_size,trainloaders[i],lr=0.001)\n",
    "\n",
    "        epo.append(curr)\n",
    "        curr=curr+1\n",
    "        print(curr)\n",
    "\n",
    "        res_acc=[]  \n",
    "        res_loss=[]        \n",
    "        for i in range(n):\n",
    "            hold=test_whole_data(mod_list[i],x_test,y_test)\n",
    "            print(hold)\n",
    "            res_acc.append(hold[1])\n",
    "            res_loss.append(hold[0])\n",
    "            \n",
    "\n",
    "\n",
    "        avg_value=sum(res_acc)/n\n",
    "        avg_loss=sum(res_loss)/n\n",
    "        print(f'avrage value {avg_value}')\n",
    "\n",
    "        acc.append([avg_value,avg_loss])    \n",
    "        if avg_value > best_avg_acc:\n",
    "            best_avg_acc = avg_value\n",
    "\n",
    "\n",
    "        parameter_lists=[]\n",
    "        k=0\n",
    "        for model in mod_list:\n",
    "            if res_acc[k]>avg_value:\n",
    "                print(\"fedder\")\n",
    "                hold=model.give()\n",
    "                parameter_lists.append(hold.parameters())\n",
    "            k=k+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        avg_params = [torch.mean(torch.stack(params), dim=0) for params in zip(*parameter_lists)]\n",
    "\n",
    "        \n",
    "\n",
    "        glob_hold = ModelA() \n",
    "\n",
    "        \n",
    "        updated_params = []\n",
    "\n",
    "        \n",
    "        for glob_param, avg_param in zip(glob_hold.parameters(), avg_params):\n",
    "            updated_params.append(avg_param)\n",
    "\n",
    "       \n",
    "        for glob_param, updated_param in zip(glob_hold.parameters(), updated_params):\n",
    "            glob_param.data.copy_(updated_param)\n",
    "\n",
    "        # Freeze all layers in glob_hold\n",
    "        if curr % 50 == 0:\n",
    "            print(\"fixer\")\n",
    "            \n",
    "            for param in glob_hold.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "\n",
    "            for model_instance in mod_list:\n",
    "                model_instance.model = glob_hold\n",
    "                \n",
    "\n",
    "      \n",
    "            k=0\n",
    "            for i in range(n):\n",
    "                if res_acc[k]<avg_value:\n",
    "                    print(\"fed_fix\")\n",
    "                    train(mod_list[i],n_epochs+500,batch_size,trainloaders[i],lr=0.002)\n",
    "                k=k+1\n",
    "\n",
    "            print(test_whole_data(mod_list[0],x_test,y_test))\n",
    "        else:\n",
    "            for model_instance in mod_list:\n",
    "                model_instance.model = glob_hold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "         \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    plt.plot(epo,acc,label=('Average Accuracy',\"Average Loss\"))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.title(\"Convergence of Federated Learning with Five Models (5 Epochs, Batch Size 32)\")\n",
    "    plt.legend()\n",
    "    plt.axes([1, 0.4, 0.2, 0.2])\n",
    "    plt.boxplot(res_acc)\n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.title('Boxplot of Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    return best_avg_acc\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_global_Imp(5,5000,5,32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_data(n):\n",
    "    data= pd.read_csv(\"Heart Attack.csv\")\n",
    "    data_start= pd.read_csv(\"Heart Attack.csv\")\n",
    "    df_chunks = np.array_split(data_start, n)\n",
    "    \n",
    "\n",
    "    trainloaders=[]\n",
    "    testloaders = []\n",
    "\n",
    "        \n",
    "    X = data.drop(columns='class')\n",
    "    y = data['class']\n",
    "\n",
    "    X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "    X_train1 = torch.Tensor(X_train1.values)\n",
    "    y_train1 = torch.Tensor(y_train1.values)\n",
    "    X_train1 = torch.tensor(X_train1, dtype=torch.float32)\n",
    "    y_train1= torch.tensor(y_train1, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    X_train1=X_train1.to(device)\n",
    "    y_train1=y_train1.to(device)\n",
    "\n",
    "    X_test1 = torch.Tensor(X_test1.values)\n",
    "    y_test1= torch.Tensor(y_test1.values)\n",
    "    X_test1 = torch.tensor(X_test1, dtype=torch.float32)\n",
    "    y_test1 = torch.tensor(y_test1, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    X_test1=X_test1.to(device)\n",
    "    y_test1=y_test1.to(device)\n",
    "        \n",
    "\n",
    "\n",
    "    for data in df_chunks:\n",
    "        hold1=[]\n",
    "        hold2=[]\n",
    "        \n",
    "        X = data.drop(columns='class')\n",
    "        y = data['class']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "        X_train = torch.Tensor(X_train.values)\n",
    "        y_train = torch.Tensor(y_train.values)\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "        X_train=X_train.to(device)\n",
    "        y_train=y_train.to(device)\n",
    "\n",
    "        X_test = torch.Tensor(X_test.values)\n",
    "        y_test= torch.Tensor(y_test.values)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "        X_test=X_test.to(device)\n",
    "        y_test=y_test.to(device)\n",
    "        \n",
    "        hold1.append((X_train,y_train))\n",
    "        hold2.append((X_test,y_test))\n",
    "        trainloaders.append(hold1)\n",
    "        testloaders.append(hold2)\n",
    "\n",
    "    return trainloaders,testloaders,X_train1,y_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(8, 8)\n",
    "        self.act1 = nn.Sigmoid()        \n",
    "        self.hidden2 = nn.Linear(8, 16)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "        self.hidden3 = nn.Linear(16, 32)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "        self.hidden4 = nn.Linear(32, 16)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "        self.hidden5 = nn.Linear(16, 8)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "        self.hidden6 = nn.Linear(8, 4)\n",
    "        self.output= nn.Linear(4, 1)\n",
    "        self.act_output = nn.Sigmoid()\n",
    "\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act2(self.hidden3(x))\n",
    "        x = self.act2(self.hidden4(x))\n",
    "        x = self.act2(self.hidden5(x))\n",
    "        x = self.act2(self.hidden6(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_model(n):\n",
    "    model_list=[]\n",
    "    for i in range(n):\n",
    "        model_list.append(Model())\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fed(n,k,n_epochs,batch_size):\n",
    "\n",
    "    mod_list=num_model(n)\n",
    "    trainloaders,testloaders,x_test,y_test=creat_data(n)\n",
    "\n",
    "\n",
    "    glob_model=Model()\n",
    "    acc=[]\n",
    "    epo=[]\n",
    "    curr=0\n",
    "    best_value=0\n",
    "    for i in range(k):\n",
    "\n",
    "        for i in range(n):\n",
    "            train(mod_list[i],n_epochs,batch_size,trainloaders[i],0.001)\n",
    "\n",
    "        res_acc=[]  \n",
    "        res_loss=[]        \n",
    "        for i in range(n):\n",
    "            hold=test_whole_data(mod_list[i],x_test,y_test)\n",
    "\n",
    "            res_acc.append(hold[0])\n",
    "            res_loss.append(hold[1])\n",
    "        \n",
    "    \n",
    "\n",
    "        parameter_lists = [model.parameters() for model in mod_list]\n",
    "        avg_acc=sum(res_acc)/n\n",
    "        if avg_acc>best_value:\n",
    "            best_value=avg_acc\n",
    "\n",
    "        avg_params = [torch.mean(torch.stack(params), dim=0) for params in zip(*parameter_lists)]\n",
    "\n",
    "        for glob_param, param in zip(glob_model.parameters(), avg_params):\n",
    "            glob_param.data.copy_(param)\n",
    "\n",
    "        \n",
    "\n",
    "        mod_list=[]\n",
    "\n",
    "        for i in range(n):\n",
    "            mod_list.append(glob_model)\n",
    "\n",
    "\n",
    "        epo.append(curr)\n",
    "        curr=+curr+1\n",
    "        print(curr)\n",
    "        acc.append(test_whole_data(mod_list[i],x_test,y_test))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.plot(epo,acc,label=('Average Accuracy',\"Average Loss\"))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.title(\"Convergence of Federated Learning with Five Models (5 Epochs, Batch Size 32)\")\n",
    "    plt.legend()\n",
    "    plt.axes([1, 0.4, 0.2, 0.2])\n",
    "    plt.boxplot(res_acc)\n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.title('Boxplot of Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    return best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\isak1\\miniconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train1 = torch.tensor(X_train1, dtype=torch.float32)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train1= torch.tensor(y_train1, dtype=torch.float32).reshape(-1, 1)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test1 = torch.tensor(X_test1, dtype=torch.float32)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test1 = torch.tensor(y_test1, dtype=torch.float32).reshape(-1, 1)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32)\n",
      "C:\\Users\\isak1\\AppData\\Local\\Temp\\ipykernel_20268\\2264630936.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_global(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m1000\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 15\u001b[0m, in \u001b[0;36mtrain_global\u001b[1;34m(n, k, n_epochs, batch_size)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m---> 15\u001b[0m         train(mod_list[i],n_epochs,batch_size,trainloaders[i],\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m     17\u001b[0m     res_acc\u001b[38;5;241m=\u001b[39m[]  \n\u001b[0;32m     18\u001b[0m     res_loss\u001b[38;5;241m=\u001b[39m[]        \n",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, n_epochs, batch_size, traindata, lr)\u001b[0m\n\u001b[0;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_func(y_pred, ybatch)\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m&\u001b[39m epoch \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\isak1\\miniconda3\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\isak1\\miniconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_fed(5,1000,5,32)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FedAvg",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
